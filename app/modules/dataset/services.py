import hashlib
import logging
import os
import shutil
import uuid
from typing import Optional

from flask import request

from app.modules.auth.services import AuthenticationService
from app.modules.dataset.models import DataSet, DSMetaData, DSMetaDataEditLog, DSViewRecord
from app.modules.dataset.repositories import (
    AuthorRepository,
    DataSetRepository,
    DOIMappingRepository,
    DSDownloadRecordRepository,
    DSMetaDataEditLogRepository,
    DSMetaDataRepository,
    DSViewRecordRepository,
)
from app.modules.dataset.validator import validate_dataset_package
from app.modules.featuremodel.repositories import FeatureModelRepository, FMMetaDataRepository
from app.modules.hubfile.repositories import (
    HubfileDownloadRecordRepository,
    HubfileRepository,
    HubfileViewRecordRepository,
)
from core.services.BaseService import BaseService

logger = logging.getLogger(__name__)


def calculate_checksum_and_size(file_path):
    file_size = os.path.getsize(file_path)
    with open(file_path, "rb") as file:
        content = file.read()
        hash_hex = hashlib.sha256(content).hexdigest()
        return hash_hex, file_size


class DataSetService(BaseService):
    def __init__(self):
        super().__init__(DataSetRepository())
        self.feature_model_repository = FeatureModelRepository()
        self.author_repository = AuthorRepository()
        self.dsmetadata_repository = DSMetaDataRepository()
        self.fmmetadata_repository = FMMetaDataRepository()
        self.dsdownloadrecord_repository = DSDownloadRecordRepository()
        self.hubfiledownloadrecord_repository = HubfileDownloadRecordRepository()
        self.hubfilerepository = HubfileRepository()
        self.dsviewrecord_repostory = DSViewRecordRepository()
        self.hubfileviewrecord_repository = HubfileViewRecordRepository()

    def move_feature_models(self, dataset: DataSet):
        current_user = AuthenticationService().get_authenticated_user()
        source_dir = current_user.temp_folder()

        working_dir = os.getenv("WORKING_DIR", "")
        dest_dir = os.path.join(working_dir, "uploads", f"user_{current_user.id}", f"dataset_{dataset.id}")

        os.makedirs(dest_dir, exist_ok=True)

        for feature_model in dataset.feature_models:
            filename = feature_model.fm_meta_data.filename
            src_path = os.path.join(source_dir, filename)
            dst_path = os.path.join(dest_dir, filename)

            if not os.path.exists(src_path):
                # If the file isn't in the user's temp folder, warn and continue.
                logger.warning(f"Source file not found, skipping move: {src_path}")
                continue
            try:
                # If destination file already exists, remove it (overwrite behaviour).
                if os.path.exists(dst_path):
                    logger.info(f"Destination file {dst_path} exists — removing before move.")
                    os.remove(dst_path)

                shutil.move(src_path, dst_path)
            except Exception as exc:
                # Log and raise so upstream code can handle rollback if needed.
                logger.exception(f"Failed moving file {src_path} to {dst_path}: {exc}")
                raise

    def get_synchronized(self, current_user_id: int) -> DataSet:
        return self.repository.get_synchronized(current_user_id)

    def get_unsynchronized(self, current_user_id: int) -> DataSet:
        return self.repository.get_unsynchronized(current_user_id)

    def get_unsynchronized_dataset(self, current_user_id: int, dataset_id: int) -> DataSet:
        return self.repository.get_unsynchronized_dataset(current_user_id, dataset_id)

    def latest_synchronized(self):
        return self.repository.latest_synchronized()

    def count_synchronized_datasets(self):
        return self.repository.count_synchronized_datasets()

    def count_feature_models(self):
        return self.feature_model_service.count_feature_models()

    def count_authors(self) -> int:
        return self.author_repository.count()

    def count_dsmetadata(self) -> int:
        return self.dsmetadata_repository.count()

    def total_dataset_downloads(self) -> int:
        return self.dsdownloadrecord_repository.total_dataset_downloads()

    def total_dataset_views(self) -> int:
        return self.dsviewrecord_repostory.total_dataset_views()

    def create_from_form(self, form, current_user) -> DataSet:
        main_author = {
            "name": f"{current_user.profile.surname}, {current_user.profile.name}",
            "affiliation": current_user.profile.affiliation,
            "orcid": current_user.profile.orcid,
        }
        try:
            logger.info(f"Creating dsmetadata...: {form.get_dsmetadata()}")

            # Obtener metadata del formulario sin publication_doi auto-generado
            dsmetadata_data = form.get_dsmetadata()

            dsmetadata = self.dsmetadata_repository.create(**dsmetadata_data)
            for author_data in [main_author] + form.get_authors():
                author = self.author_repository.create(commit=False, ds_meta_data_id=dsmetadata.id, **author_data)
                dsmetadata.authors.append(author)

            dataset = self.create(commit=False, user_id=current_user.id, ds_meta_data_id=dsmetadata.id)

            # Procesar archivos CSV del dataset
            # Nota: Aunque el modelo se llama "feature_model", ahora representa archivos CSV de datos climáticos
            uploaded_filenames = []
            for csv_file_form in form.feature_models:
                filename = csv_file_form.filename.data

                # Los archivos CSV no necesitan publication_doi individual, solo metadata básica
                fmmetadata_data = csv_file_form.get_fmmetadata()

                fmmetadata = self.fmmetadata_repository.create(commit=False, **fmmetadata_data)
                for author_data in csv_file_form.get_authors():
                    author = self.author_repository.create(commit=False, fm_meta_data_id=fmmetadata.id, **author_data)
                    fmmetadata.authors.append(author)

                # Crear entrada de feature_model (representa un archivo CSV)
                fm = self.feature_model_repository.create(
                    commit=False, data_set_id=dataset.id, fm_meta_data_id=fmmetadata.id
                )
                uploaded_filenames.append(csv_file_form.filename.data)

            file_paths = [os.path.join(current_user.temp_folder(), fn) for fn in uploaded_filenames]
            try:
                # Validar que todos los archivos CSV tengan la estructura correcta
                validate_dataset_package(file_paths=file_paths)

            except Exception:
                # rollback y propaga error (tu código ya maneja rollback en el except general)
                self.repository.session.rollback()
                raise

            for e in range(0, len(uploaded_filenames)):
                filename = uploaded_filenames[e]

                file_path = file_paths[e]

                checksum, size = calculate_checksum_and_size(file_path)

                file = self.hubfilerepository.create(
                    commit=False, name=filename, checksum=checksum, size=size, feature_model_id=fm.id
                )
                fm.files.append(file)
            self.repository.session.commit()
        except Exception as exc:
            logger.info(f"Exception creating dataset from form...: {exc}")
            self.repository.session.rollback()
            raise exc
        return dataset

    def update_dsmetadata(self, id, **kwargs):
        return self.dsmetadata_repository.update(id, **kwargs)

    def get_uvlhub_doi(self, dataset: DataSet) -> str:
        domain = os.getenv("DOMAIN", "localhost")
        return f"http://{domain}/doi/{dataset.ds_meta_data.dataset_doi}"


class AuthorService(BaseService):
    def __init__(self):
        super().__init__(AuthorRepository())


class DSDownloadRecordService(BaseService):
    def __init__(self):
        super().__init__(DSDownloadRecordRepository())


class DSMetaDataService(BaseService):
    def __init__(self):
        super().__init__(DSMetaDataRepository())

    def update(self, id, **kwargs):
        return self.repository.update(id, **kwargs)

    def filter_by_doi(self, doi: str) -> Optional[DSMetaData]:
        return self.repository.filter_by_doi(doi)


class DSViewRecordService(BaseService):
    def __init__(self):
        super().__init__(DSViewRecordRepository())

    def the_record_exists(self, dataset: DataSet, user_cookie: str):
        return self.repository.the_record_exists(dataset, user_cookie)

    def create_new_record(self, dataset: DataSet, user_cookie: str) -> DSViewRecord:
        return self.repository.create_new_record(dataset, user_cookie)

    def create_cookie(self, dataset: DataSet) -> str:
        user_cookie = request.cookies.get("view_cookie")
        if not user_cookie:
            user_cookie = str(uuid.uuid4())

        existing_record = self.the_record_exists(dataset=dataset, user_cookie=user_cookie)

        if not existing_record:
            self.create_new_record(dataset=dataset, user_cookie=user_cookie)

        return user_cookie


class DOIMappingService(BaseService):
    def __init__(self):
        super().__init__(DOIMappingRepository())

    def get_new_doi(self, old_doi: str) -> str:
        doi_mapping = self.repository.get_new_doi(old_doi)
        if doi_mapping:
            return doi_mapping.dataset_doi_new
        else:
            return None


class SizeService:
    def __init__(self):
        pass

    def get_human_readable_size(self, size: int) -> str:
        if size < 1024:
            return f"{size} bytes"
        elif size < 1024**2:
            return f"{round(size / 1024, 2)} KB"
        elif size < 1024**3:
            return f"{round(size / (1024 ** 2), 2)} MB"
        else:
            return f"{round(size / (1024 ** 3), 2)} GB"


class DSMetaDataEditLogService(BaseService):
    def __init__(self):
        super().__init__(DSMetaDataEditLogRepository())

    def log_edit(
        self,
        ds_meta_data_id: int,
        user_id: int,
        field_name: str,
        old_value: str,
        new_value: str,
        change_summary: str = None,
    ) -> DSMetaDataEditLog:
        return self.repository.create(
            ds_meta_data_id=ds_meta_data_id,
            user_id=user_id,
            field_name=field_name,
            old_value=old_value,
            new_value=new_value,
            change_summary=change_summary,
        )

    def log_multiple_edits(self, ds_meta_data_id: int, user_id: int, changes: list) -> list:
        logs = []
        for change in changes:
            log = self.log_edit(
                ds_meta_data_id=ds_meta_data_id,
                user_id=user_id,
                field_name=change.get("field"),
                old_value=change.get("old"),
                new_value=change.get("new"),
                change_summary=change.get("summary"),
            )
            logs.append(log)
        return logs

    def get_changelog(self, ds_meta_data_id: int) -> list:
        return self.repository.get_by_ds_meta_data_id(ds_meta_data_id)

    def get_changelog_by_dataset_id(self, dataset_id: int) -> list:
        return self.repository.get_by_dataset_id(dataset_id)
